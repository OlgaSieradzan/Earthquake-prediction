{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predykcja cen akcji spółki Microsoft z wykorzystaniem LSTM (ang. Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Niezbędne biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import *\n",
    "from keras.api.optimizers import *\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable eager execution\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbieranie danych\n",
    " * Zakres dat: od `01-01-2010` do `01-01-2024`\n",
    " * Oznaczenie spółki: `MSFT.US`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik ./msft_us_historical_data.csv już istnieje\n",
      "         Date     Open     High      Low    Close        Volume\n",
      "0  2010-01-04  24.0854  24.4389  24.0358  24.3178  4.886916e+07\n",
      "1  2010-01-05  24.2542  24.4389  24.0765  24.3267  6.331174e+07\n",
      "2  2010-01-06  24.2642  24.4230  23.9852  24.1758  7.403254e+07\n",
      "3  2010-01-07  24.0537  24.1232  23.7221  23.9257  6.434577e+07\n",
      "4  2010-01-08  23.7837  24.2642  23.7608  24.0934  6.515561e+07\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3522 entries, 0 to 3521\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    3522 non-null   object \n",
      " 1   Open    3522 non-null   float64\n",
      " 2   High    3522 non-null   float64\n",
      " 3   Low     3522 non-null   float64\n",
      " 4   Close   3522 non-null   float64\n",
      " 5   Volume  3522 non-null   float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 165.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ścieżka do pliku CSV w repozytorium\n",
    "csv_path = './msft_us_historical_data.csv'\n",
    "\n",
    "# Sprawdzenie, czy plik istnieje\n",
    "if not os.path.exists(csv_path):\n",
    "    # URL do pobrania danych historycznych dla MSFT.US\n",
    "    url = 'https://stooq.com/q/d/l/?s=msft.us&i=d&d1=20100101&d2=20240101'\n",
    "    \n",
    "    # Pobieranie danych\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Sprawdzenie, czy żądanie się powiodło\n",
    "    if response.status_code == 200:\n",
    "        # Zapisanie danych do pliku CSV w repozytorium\n",
    "        with open(csv_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \n",
    "        print(f'Dane zostały pobrane i zapisane do {csv_path}')\n",
    "    else:\n",
    "        print(f'Błąd podczas pobierania danych: {response.status_code}')\n",
    "else:\n",
    "    print(f'Plik {csv_path} już istnieje')\n",
    "\n",
    "# Wczytanie danych do DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Wyświetlenie pierwszych kilku wierszy\n",
    "print(df.head())\n",
    "\n",
    "# Wyświetlenie podstawowych informacji o danych\n",
    "print(df.info())\n",
    "\n",
    "# Zmiana typu kolumny 'Date' na typ daty\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybór kolumny 'Close' do predykcji\n",
    "data = df['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalizacja danych\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Tworzenie sekwencji danych\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        labels.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "# Podział na zbiór treningowy i testowy\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "y_temp = data[split + seq_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja do budowy i trenowania modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(layers=3, neurons=20, optimizer_class=Adam, optimizer_params={'learning_rate': 0.001}, activation='linear', epochs=50, batch_size=16):\n",
    "    # Tworzenie modelu sekwencyjnego\n",
    "    model = Sequential()\n",
    "\n",
    "    # Dodawanie warstw LSTM\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            # Pierwsza warstwa LSTM z określonymi neuronami i kształtem wejściowym\n",
    "            model.add(Input(shape=(seq_length, 1)))\n",
    "        elif i == layers - 1:\n",
    "            # Ostatnia warstwa LSTM bez zwracania sekwencji\n",
    "            model.add(LSTM(neurons))\n",
    "        else:\n",
    "            # Środkowe warstwy LSTM zwracające sekwencje\n",
    "            model.add(LSTM(neurons, return_sequences=True))\n",
    "    # Dodawanie warstwy wyjściowej Dense        \n",
    "    model.add(Dense(1, activation=activation))\n",
    "    \n",
    "    # Kompilacja modelu z określonym optymalizatorem i funkcją straty\n",
    "    optimizer = optimizer_class(**optimizer_params)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Trenowanie modelu na danych treningowych z walidacją na danych testowych\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    # Zwracanie wytrenowanego modelu i historii trenowania\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja do rysowania wykresów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, data, test_data, predicted_data, title, history, file_name, folder_name):\n",
    "    # Tworzenie folderu, jeśli nie istnieje\n",
    "    folder_name = 'plots/' + folder_name\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Tworzenie wykresu z trzema podwykresami\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 15))\n",
    "\n",
    "    # Wykres podstawowego zbioru danych do momentu prognozowania\n",
    "    ax1.plot(df['Date'][:split + seq_length], data[:split + seq_length], color='blue', label='Rzeczywiste ceny', linewidth=1)\n",
    "    ax1.plot(df['Date'][split + seq_length:], test_data, color='orange', linestyle='dashed', label='Okres testowy', linewidth=1)\n",
    "    ax1.plot(df['Date'], predicted_data, color='red', linestyle='dotted', label='Prognozowane ceny', linewidth=2)\n",
    "    ax1.set_xlabel('Data')\n",
    "    ax1.set_ylabel('Cena zamknięcia')\n",
    "    ax1.set_title(f'Prognozowanie cen akcji MSFT.US - {title}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Wykres przybliżenia okresu testowego i prognoz\n",
    "    ax2.plot(df['Date'][split + seq_length:], test_data, color='orange', linestyle='dashed', label='Okres testowy', linewidth=1)\n",
    "    ax2.plot(df['Date'][split + seq_length:], predicted_data[split + seq_length:], color='red', linestyle='dotted', label='Prognozowane ceny', linewidth=2)\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel('Cena zamknięcia')\n",
    "    ax2.set_title('Prognozowanie cen akcji MSFT.US - Okres testowy i prognozy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Wykres błędów trenowania i walidacji\n",
    "    ax3.plot(history.history['loss'], label='Błąd trenowania')\n",
    "    ax3.plot(history.history['val_loss'], label='Błąd walidacji')\n",
    "    ax3.set_xlabel('Epoka')\n",
    "    ax3.set_ylabel('Błąd')\n",
    "    ax3.set_title('Błąd trenowania i walidacji')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Zapisywanie wykresu do pliku\n",
    "    plot_path = os.path.join(folder_name, f'{file_name}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Określenie parametrów do testów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametry do przetestowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_options = [2, 3, 4, 5]\n",
    "neurons_options = [50, 100, 150, 200]\n",
    "optimizers_options = [Adam, RMSprop, SGD, Adagrad]\n",
    "activations_options = ['tanh', 'sigmoid', 'linear', 'relu']\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "# Przechowywanie wyników\n",
    "results = []\n",
    "\n",
    "# Liczba powtórzeń dla każdego zestawu parametrów\n",
    "num_repeats = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie wpływu liczby warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n"
     ]
    }
   ],
   "source": [
    "for layers in layers_options:\n",
    "    for i in range(num_repeats):\n",
    "        K.clear_session()\n",
    "        \n",
    "        optimizer_params = {'learning_rate': 0.001}\n",
    "        model_layers, history_layers = build_and_train_model(layers=layers, optimizer_class=RMSprop, optimizer_params=optimizer_params)\n",
    "        loss_layers = history_layers.history['val_loss'][-1]\n",
    "        \n",
    "        # Predykcja na zbiorze testowym\n",
    "        predicted_layers = model_layers.predict(X_test)\n",
    "        predicted_layers = scaler.inverse_transform(predicted_layers.reshape(-1, 1))\n",
    "        \n",
    "        # Oblicz RMSE i R²\n",
    "        rmse_layers = np.sqrt(mean_squared_error(y_temp, predicted_layers))\n",
    "        r2_layers = r2_score(y_temp, predicted_layers)\n",
    "\n",
    "        results.append(('layers', layers, loss_layers, rmse_layers, r2_layers))\n",
    "\n",
    "        # Przygotowanie danych do wykresu\n",
    "        predicted_data_layers = np.empty_like(data)\n",
    "        predicted_data_layers[:, :] = np.nan\n",
    "        predicted_data_layers[split + seq_length:] = predicted_layers\n",
    "        \n",
    "        # Rysowanie wykresu\n",
    "        # plot_results(df, data, data[split + seq_length:], predicted_data_layers, f'Liczba warstw: {layers}', history_layers, f'layers_{layers}_rep_{i}', 'layers')\n",
    "\n",
    "with pd.ExcelWriter('results.xlsx') as writer:\n",
    "    df = pd.DataFrame(results, columns=['param', 'value', 'loss', 'rmse', 'R2'])\n",
    "    df.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie wpływu liczby neuronów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n"
     ]
    }
   ],
   "source": [
    "results.clear()\n",
    "\n",
    "for neurons in neurons_options:\n",
    "    for i in range(num_repeats):\n",
    "        K.clear_session()\n",
    "        \n",
    "        optimizer_params = {'learning_rate': 0.001}\n",
    "        model_neurons, history_neurons = build_and_train_model(neurons=neurons, optimizer_class=RMSprop, optimizer_params=optimizer_params)\n",
    "        loss_neurons = history_neurons.history['val_loss'][-1]\n",
    "        \n",
    "        # Predykcja na zbiorze testowym\n",
    "        predicted_neurons = model_neurons.predict(X_test)\n",
    "        predicted_neurons = scaler.inverse_transform(predicted_neurons.reshape(-1, 1))\n",
    "\n",
    "        # Oblicz RMSE i R²\n",
    "        rmse_neurons = np.sqrt(mean_squared_error(y_temp, predicted_neurons))\n",
    "        r2_neurons = r2_score(y_temp, predicted_neurons)\n",
    "        results.append(('neurons', neurons, loss_neurons, rmse_neurons, r2_neurons))\n",
    "        \n",
    "        # Przygotowanie danych do wykresu\n",
    "        predicted_data_neurons = np.empty_like(data)\n",
    "        predicted_data_neurons[:, :] = np.nan\n",
    "        predicted_data_neurons[split + seq_length:] = predicted_neurons\n",
    "        \n",
    "        # Rysowanie wykresu\n",
    "        # plot_results(df, data, data[split + seq_length:], predicted_data_neurons, f'Liczba neuronów: {neurons}_{i}', history_neurons, f'neurons_{neurons}_rep_{i}', 'neurons')\n",
    "\n",
    "with pd.ExcelWriter('results.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    results_df = pd.DataFrame(results, columns=['Parameter', 'Value', 'Loss', 'RMSE', 'R²'])\n",
    "    results_df.to_excel(writer, sheet_name='neurons', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie wpływu optymalizatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n"
     ]
    }
   ],
   "source": [
    "results.clear()\n",
    "\n",
    "for optimizer in optimizers_options:\n",
    "    for i in range(num_repeats):\n",
    "        K.clear_session()\n",
    "        \n",
    "        optimizer_params = {'learning_rate': 0.001}\n",
    "        model_optimizer, history_optimizer = build_and_train_model(optimizer_class=optimizer, optimizer_params=optimizer_params)\n",
    "        loss_optimizer = history_optimizer.history['val_loss'][-1]\n",
    "        \n",
    "        # Predykcja na zbiorze testowym\n",
    "        predicted_optimizer = model_optimizer.predict(X_test)\n",
    "        predicted_optimizer = scaler.inverse_transform(predicted_optimizer.reshape(-1, 1))\n",
    "        \n",
    "        # Oblicz RMSE i R²\n",
    "        rmse_optimizer = np.sqrt(mean_squared_error(y_temp, predicted_optimizer))   \n",
    "        r2_optimizer = r2_score(y_temp, predicted_optimizer)\n",
    "        results.append(('optimizer', optimizer.__name__, loss_optimizer, rmse_optimizer, r2_optimizer))\n",
    "\n",
    "        # Przygotowanie danych do wykresu\n",
    "        predicted_data_optimizer = np.empty_like(data)\n",
    "        predicted_data_optimizer[:, :] = np.nan\n",
    "        predicted_data_optimizer[split + seq_length:] = predicted_optimizer\n",
    "        \n",
    "        # Rysowanie wykresu\n",
    "        # plot_results(df, data, data[split + seq_length:], predicted_data_optimizer, f'Optymalizator: {optimizer.__name__}_{i}', history_optimizer, f'optimizer_{optimizer.__name__}_rep_{i}', 'optimizer')\n",
    "\n",
    "with pd.ExcelWriter('results.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    results_df = pd.DataFrame(results, columns=['Parameter', 'Value', 'Loss', 'RMSE', 'R²'])\n",
    "    results_df.to_excel(writer, sheet_name='optimizer', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie wpływu funkcji aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "results.clear()\n",
    "\n",
    "for activation in activations_options:\n",
    "    for i in range(num_repeats):\n",
    "        K.clear_session()\n",
    "        \n",
    "        optimizer_params = {'learning_rate': 0.001}\n",
    "        model_activation, history_activation = build_and_train_model(optimizer_class=RMSprop, optimizer_params=optimizer_params, activation=activation)\n",
    "        loss_activation = history_activation.history['val_loss'][-1]\n",
    "\n",
    "        # Predykcja na zbiorze testowym\n",
    "        predicted_activation = model_activation.predict(X_test)\n",
    "        predicted_activation = scaler.inverse_transform(predicted_activation.reshape(-1, 1))\n",
    "        \n",
    "        # Oblicz RMSE i R²\n",
    "        rmse_activation = np.sqrt(mean_squared_error(y_temp, predicted_activation))\n",
    "        r2_activation = r2_score(y_temp, predicted_activation)\n",
    "        results.append(('activation', activation, loss_activation, rmse_activation, r2_activation))\n",
    "\n",
    "        # Przygotowanie danych do wykresu\n",
    "        predicted_data_activation = np.empty_like(data)\n",
    "        predicted_data_activation[:, :] = np.nan\n",
    "        predicted_data_activation[split + seq_length:] = predicted_activation\n",
    "        \n",
    "        # Rysowanie wykresu\n",
    "        # plot_results(df, data, data[split + seq_length:], predicted_data_activation, f'Funkcja aktywacji: {activation}_{i}', history_activation, f'activation_{activation}_rep_{i}', 'activation')\n",
    "\n",
    "with pd.ExcelWriter('results.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    results_df = pd.DataFrame(results, columns=['Parameter', 'Value', 'Loss', 'RMSE', 'R²'])\n",
    "    results_df.to_excel(writer, sheet_name='activation', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testowanie wpływu wielkości batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n"
     ]
    }
   ],
   "source": [
    "results.clear()\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for i in range(num_repeats):\n",
    "        K.clear_session()\n",
    "        \n",
    "        optimizer_params = {'learning_rate': 0.001}\n",
    "        model_batch_size, history_batch_size = build_and_train_model(optimizer_class=RMSprop, optimizer_params=optimizer_params, batch_size=batch_size)\n",
    "        loss_batch_size = history_batch_size.history['val_loss'][-1]\n",
    "\n",
    "        # Predykcja na zbiorze testowym\n",
    "        predicted_batch_size = model_batch_size.predict(X_test)\n",
    "        predicted_batch_size = scaler.inverse_transform(predicted_batch_size.reshape(-1, 1))\n",
    "\n",
    "        # Oblicz RMSE i R²\n",
    "        rmse_batch_size = np.sqrt(mean_squared_error(y_temp, predicted_batch_size))\n",
    "        r2_batch_size = r2_score(y_temp, predicted_batch_size)\n",
    "        results.append(('batch_size', batch_size, loss_batch_size, rmse_batch_size, r2_batch_size))\n",
    "\n",
    "        # Przygotowanie danych do wykresu\n",
    "        predicted_data_batch_size = np.empty_like(data)\n",
    "        predicted_data_batch_size[:, :] = np.nan\n",
    "        predicted_data_batch_size[split + seq_length:] = predicted_batch_size\n",
    "\n",
    "        # Rysowanie wykresu\n",
    "        # plot_results(df, data, data[split + seq_length:], predicted_data_batch_size, f'Batch size: {batch_size}_{i}', history_batch_size, f'batch_size_{batch_size}_rep_{i}', 'batch_size')\n",
    "\n",
    "with pd.ExcelWriter('results.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    results_df = pd.DataFrame(results, columns=['Parameter', 'Value', 'Loss', 'RMSE', 'R²'])\n",
    "    results_df.to_excel(writer, sheet_name='batch_size', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
