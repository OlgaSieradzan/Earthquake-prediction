{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dataset to C:\\Users\\ameli\\.cache\\kagglehub\\datasets\\usgs\\earthquake-database\\versions\\1\n",
      "File moved from C:\\Users\\ameli\\.cache\\kagglehub\\datasets\\usgs\\earthquake-database\\versions\\1 to ./dataset\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"usgs/earthquake-database\")\n",
    "dest_path = \"./dataset\"\n",
    "\n",
    "# Move to repo\n",
    "shutil.move(path, dest_path)\n",
    "print(f\"File moved from {path} to {dest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                              0\n",
      "Time                              0\n",
      "Latitude                          0\n",
      "Longitude                         0\n",
      "Type                              0\n",
      "Depth                             0\n",
      "Depth Error                   18951\n",
      "Depth Seismic Stations        16315\n",
      "Magnitude                         0\n",
      "Magnitude Type                    3\n",
      "Magnitude Error               23085\n",
      "Magnitude Seismic Stations    20848\n",
      "Azimuthal Gap                 16113\n",
      "Horizontal Distance           21808\n",
      "Horizontal Error              22256\n",
      "Root Mean Square               6060\n",
      "ID                                0\n",
      "Source                            0\n",
      "Location Source                   0\n",
      "Magnitude Source                  0\n",
      "Status                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data\n",
    "# Assuming you have a CSV file with earthquake data\n",
    "data = pd.read_csv('./dataset/database.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "# print(data.head())\n",
    "\n",
    "# Check NA values\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "# Assuming you have a CSV file with earthquake data\n",
    "data = pd.read_csv('./dataset/database.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocess the data (example: fill missing values, encode categorical variables, etc.)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('target_column', axis=1)  # Replace 'target_column' with the actual target column name\n",
    "y = data['target_column']\n",
    "\n",
    "# Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the Neural Network Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Assuming a regression problem\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the Model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
